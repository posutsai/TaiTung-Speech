{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Speech in TaiTung\n",
    "\n",
    "Author: [蔡岳霖](https://www.facebook.com/profile.php?id=100002152027860) \n",
    "\n",
    "<span style=\"color: red; font-size: 40px;\">server IP: 210.240.162.7:8080<br>\n",
    "passwd: mlisfun</span>\n",
    "\n",
    "![our dataset](https://www.tensorflow.org/images/mnist_digits.png)\n",
    "![workflow](https://guillaumebrg.files.wordpress.com/2016/01/mnist_and_mlp1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "###### import modules we need ######\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing (TensorFlow already normalized)\n",
    "> Machine Learning standard steps <br>\n",
    "> 1. collect data <br>\n",
    "    >>一個好的 data 會決定你的 model 的好壞，如何根據模型的實際應用場景去決定我們要使用怎樣的 data 才是最重要的\n",
    "> 2. preprocess data <br>\n",
    "    >>實際操作 data 的時候可能會發生很多的問題:\n",
    "    * 資料缺失: 在資料內有很多的缺失，可能是 data 收集上的問題或是很多很多其他的問題\n",
    "    * 資料清洗: 如何在所收集到的資料內剔除我們不需要的資料\n",
    "    <img src=\"./bias_dataset.png\" alt=\"bias_dataset\" style=\"width: 400px;\"/>\n",
    "> 3. transform data <br>\n",
    "    >>每一維的 data 尺度可能不太一樣\n",
    "    <img src=\"https://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_89_0.png\" alt=\"normalization\" style=\"width: 400px\">\n",
    "\n",
    "[註]因為 tensorflow 提供給我們的 dataset 是已經經過 scaled 所以我們把乘回來去模擬沒有經過預處理的資料\n",
    "1. without preprocessing (train \\*= 255, test \\*= 255)\n",
    "2. with preprocessing (Tensorflow already done!)\n",
    "3. zscoring i.e. standardization \n",
    "$$z=\\frac{x-mean}{std}$$\n",
    "[results](#result_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    ### with Min-Max scaling\n",
    "    return data\n",
    "\n",
    "    ### without preprocessing ###\n",
    "    data = data * 255\n",
    "    return data\n",
    "    \n",
    "    ### zscore\n",
    "    data = stats.zscore(data, axis=1, ddof=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network structure\n",
    "> 理論上變數越多會越更大的能力去描繪一個抽象狀態，再者模型的結構代表的是你可以從上一層得到的資訊量\n",
    "> 1. inverted triangle shape:\n",
    "    * set n_node_in_NN = [64, 1024]\n",
    "    * 倒三角形的模型結構會有的效果是資訊的量逐漸擴大所以在目前的應用上較不適合 [註]相較於 auto-encoder\n",
    "> 2. pyramid shape:\n",
    "    * set n_node_in_NN = [64, 32]\n",
    "    * 逐漸透過模型的架構去做到壓縮有效資訊的效果，萃取出對於分類最有效的資訊，相較於第一種的倒三角形則提供了很多自己生成的無謂雜訊\n",
    "> 3. shallow shape:\n",
    "    * set n_node_in_NN = [64]\n",
    "    * 可以看到之前說的色鉛筆的例子，這裡只用一層沒辦法像第二種那麼有能力的描繪抽象特徵\n",
    "> 4. fat shape: optional\n",
    "    * set n_node_in_NN = [96]\n",
    "    * 同樣的變數量可以明顯看到 <font color=\"red\">胖胖網路</font> 就比 <font color=\"red\">深度網路</font> 來的沒那麼有能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### A group inverted triangle shape ### \n",
    "# n_node_in_NN = [64, 1024]\n",
    "### B group pyramid shape ###\n",
    "# n_ndoe_in_NN = [64, 32]\n",
    "### C group shallow network structure ###\n",
    "n_node_in_NN = [64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "> 激發函數的差異\n",
    "> 1. Sigmoid function:\n",
    "    >> \n",
    "    ```python\n",
    "    activation_function = tf.sigmoid\n",
    "    ```\n",
    "    >> * risk of 'gradient vanishing'\n",
    "    >> <img src=\"https://cdn-images-1.medium.com/max/800/1*8JJ6sYleUtvUZR7TOyyFVg.png\" style=\"width: 450px\">\n",
    "    >> <img src=\"http://www.ai-junkie.com/ann/evolved/images/sigmoid.jpg\" style=\"width: 200px\">\n",
    "    >> ----------------\n",
    "    >> [註] there is also 'gradient explosion'\n",
    "    >> It often occur in RNN training\n",
    "    >> <img src=\"http://kordinglab.com/lab_teaching_2016/session_6/images/cliff.png\" style=\"width: 400px\">\n",
    "> 2. ReLU:\n",
    "    >> \n",
    "    ``` python\n",
    "    activation_function = tf.nn.relu\n",
    "    ```\n",
    "    >> * design for solving vanishing gradient\n",
    "    >> * But!! it is not differentiable\n",
    "    >> <img src=\"http://cs231n.github.io/assets/nn1/relu.jpeg\" style=\"width: 300px\">\n",
    "> 3. tanh:\n",
    "    >>\n",
    "    ``` python\n",
    "    activation_function = tf.tanh\n",
    "    ```\n",
    "    >> notice the upper bond and lower bound\n",
    "    >> <img src=\"http://mathworld.wolfram.com/images/interactive/TanhReal.gif\" style=\"width: 300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### A group use sigmoid function ### \n",
    "# activation_function = tf.sigmoid\n",
    "### B group use ReLU ###\n",
    "activation_function = tf.nn.relu\n",
    "### C group use hyperbolic tangent ###\n",
    "# activation_function = tf.tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "> 測量錯誤的方式\n",
    "> 1. cross entropy\n",
    "> <img src=\"https://i.stack.imgur.com/gYMhc.png\" style=\"width: 500px\">\n",
    "> 2. mean square error\n",
    "> <img src=\"http://www.aoml.noaa.gov/hrd/Landsea/artificial/equa-4\" style=\"width: 300px\">\n",
    "[here to change code](#cost_function_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "> Methods of preventing overfitting\n",
    "> 隨機的拔掉一些 weighting 使模型不至於太過依賴一些固定的抽象特徵\n",
    "<img src=\"https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blogs/imgs/n7-1.png\" style=\"width: 450px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### A group dropout = 0.1 ### \n",
    "#dropout = 0.1\n",
    "### B group dropout = 0.5 ###\n",
    "#dropout = 0.5\n",
    "### C group dropout = 0.9 ###\n",
    "dropout = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "> another method to prevent from overfitting\n",
    "<img src=\"https://image.slidesharecdn.com/multinomialclassificationapplicationofml-170117162000/95/multinomial-classification-and-application-of-ml-38-638.jpg?cb=1484670145\" style=\"width: 700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = 0.001\n",
    "### A group l1 regularization beta = 0.001 ### \n",
    "#regularizer = tf.contrib.layers.l1_regularizer(scale=beta)\n",
    "### B group l2 regularization beta = 0.001 ###\n",
    "#regularizer = tf.contrib.layers.l2_regularizer(scale=beta)\n",
    "### C group l2 regularization beta = 0.1 ###\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "> 代表在找尋極值的過程中一步的大小\n",
    "> 1. learning_rate = 0.1\n",
    "> 2. learning_rate = 0.5\n",
    "> 3. learning_rate = 0.05\n",
    "<img src=\"https://image.slidesharecdn.com/dl-161220080238/95/learning-deep-learning-36-638.jpg?cb=1482221026\" style=\"width: 450px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### A group learning = 0.001 ### \n",
    "# learning_rate = 0.001\n",
    "### B group dropout = 0.05 ###\n",
    "learning_rate = 0.05\n",
    "### C group dropout = 0.5 ###\n",
    "# learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Batch\n",
    "> how often do we update weightings\n",
    "> 1. batch_size = 128\n",
    "> 2. batch_size = 256\n",
    "> 3. batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### A group batch_size = 128 ### \n",
    "batch_size = 128\n",
    "### B group batch_size = 256 ###\n",
    "# batch_size = 256\n",
    "### C group batch_size = 512 ###\n",
    "# batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_layer(input_tensor, input_size, output_size, activation_function=None, dropout=None):\n",
    "    weights = tf.Variable(tf.random_normal([input_size, output_size], ))\n",
    "    biases = tf.Variable(tf.random_normal([output_size],))\n",
    "    w_plus_b = tf.add(tf.matmul(input_tensor, weights), biases)\n",
    "    outputs = w_plus_b\n",
    "    if activation_function is not None:\n",
    "        outputs = activation_function(w_plus_b)\n",
    "    if dropout is not None:\n",
    "        outputs = tf.nn.dropout(outputs, dropout)\n",
    "    return outputs, weights\n",
    "\n",
    "def multilayer_perceptron(input_tensor, n_features_in_layer, dropout):\n",
    "    global regularizer\n",
    "    global activation_function\n",
    "    global w_list\n",
    "    n_input = 784\n",
    "    n_classes = 10\n",
    "    activation_function = tf.nn.relu\n",
    "    assert len(n_features_in_layer) > 0\n",
    "    w_list = []\n",
    "    temp, weight = add_layer(input_tensor, n_input, n_features_in_layer[0], activation_function, dropout)\n",
    "    w_list.append(weight)\n",
    "    for index, layer in enumerate(n_features_in_layer[0:-1]):\n",
    "        temp, weight = add_layer(temp, layer, n_features_in_layer[index + 1], activation_function, dropout)\n",
    "        w_list.append(weight)\n",
    "    output, weight = add_layer(temp, n_features_in_layer[-1], n_classes, activation_function=None, dropout=None)\n",
    "    w_list.append(weight)\n",
    "    reg_penalty = tf.contrib.layers.apply_regularization(regularizer, w_list)\n",
    "    return output, reg_penalty, w_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global pred\n",
    "    global w_list\n",
    "    y_pre = sess.run(pred, feed_dict={x: v_xs, keep_prob: 1.})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={x: v_xs, y: v_ys})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = 15\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "x = tf.py_func(preprocessing, [x], tf.float32)\n",
    "\n",
    "\n",
    "# Construct model\n",
    "pred, reg_penalty, w_list = multilayer_perceptron(x, n_node_in_NN, keep_prob)\n",
    "\n",
    "# Define loss and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cost_function_cell\"></a>\n",
    "Change cost function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### here to decide which loss function to apply ###\n",
    "## group A & C cross entropy\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)+ reg_penalty)\n",
    "## group B mean square error\n",
    "# cost = tf.reduce_mean(tf.losses.mean_squared_error(predictions=pred, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 6.304539429\n",
      "Epoch: 0002 cost= 0.832640138\n",
      "Epoch: 0003 cost= 0.656302478\n",
      "Epoch: 0004 cost= 0.676294748\n",
      "Epoch: 0005 cost= 0.709306080\n",
      "Epoch: 0006 cost= 0.757234528\n",
      "Epoch: 0007 cost= 0.725782213\n",
      "Epoch: 0008 cost= 0.704050334\n",
      "Epoch: 0009 cost= 0.714715863\n",
      "Epoch: 0010 cost= 0.718932539\n",
      "Epoch: 0011 cost= 0.734104404\n",
      "Epoch: 0012 cost= 0.708571706\n",
      "Epoch: 0013 cost= 0.724175113\n",
      "Epoch: 0014 cost= 0.690640020\n",
      "Epoch: 0015 cost= 0.724696021\n",
      "Optimization Finished!\n",
      "Accuracy: 0.8957\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "#with tf.Session() as sess:\n",
    "sess.run(init)\n",
    "\n",
    "# Training cycle\n",
    "batch_cost = []\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        avg_cost += c / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        batch_cost.append(avg_cost)\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "            \"{:.9f}\".format(avg_cost))\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", compute_accuracy(mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"result_cell\"></a>\n",
    "here is the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f65b882d828>]"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIBhe1MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65b896c9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_cost = np.array(batch_cost)\n",
    "batch = np.arange(0, len(batch_cost), 1)\n",
    "plt.plot(batch, batch_cost, 'k-', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
